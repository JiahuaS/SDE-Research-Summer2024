{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ed52f6",
   "metadata": {},
   "source": [
    "# <center>Research Work on Data Part and Theory Part<center>\n",
    "### <center>Jiahua Song<center>\n",
    "\n",
    "### This is the research work on research of Research Project on new method of Stochastic Differential Equation Symbolic Regression and Application in Quantitative Finance\n",
    "\n",
    "### Task: \n",
    "    [DONE]1: Homogeneous/Nonhomogeneous method;\n",
    "    [In Progress]2: Influence Method building; \n",
    "    3: Try some possible generating methods in the spirit of related data-driven papers, determine one method that is explainable and present the experiment in fabricated data and some desired real data; \n",
    "    4: Present the experiment in fabricated data and some desired real data;\n",
    "    5: Determine the Theorems and lemma needed in numerical SDE and SDE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1006fbb6",
   "metadata": {},
   "source": [
    "# Text Transforming Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5bc29",
   "metadata": {},
   "source": [
    "I will use a simple math example to illustrate the transformation of a sentence into a numerical representation (embdedding) using the principle of Transformer Architecture. \n",
    "\n",
    "### Example Sentence\n",
    "Let's take a simple sentence:\n",
    "```\n",
    "\"I love NLP\"\n",
    "```\n",
    "\n",
    "### Step 1: Tokenization\n",
    "First, we tokenize the sentence. For simplicity, we'll use word-level tokenization:\n",
    "```\n",
    "Tokens: [\"I\", \"love\", \"NLP\"]\n",
    "```\n",
    "\n",
    "### Step 2: Word Embeddings\n",
    "Each token is mapped to a dense vector using pre-trained word embeddings. Suppose our embedding dimension is 4, and we have the following vectors for each word (these vectors are for illustration purposes only):\n",
    "\n",
    "```\n",
    "Embedding(\"I\")    = [0.2, 0.1, 0.4, 0.7]\n",
    "Embedding(\"love\") = [0.9, 0.3, 0.2, 0.8]\n",
    "Embedding(\"NLP\")  = [0.4, 0.5, 0.6, 0.1]\n",
    "```\n",
    "\n",
    "### Step 3: Positional Encoding\n",
    "We add positional encodings to these embeddings to incorporate the order information. Suppose our positional encoding vectors are:\n",
    "\n",
    "```\n",
    "PosEncoding(1) = [0.1, 0.0, 0.1, 0.0]\n",
    "PosEncoding(2) = [0.0, 0.1, 0.0, 0.1]\n",
    "PosEncoding(3) = [0.1, 0.1, 0.1, 0.1]\n",
    "```\n",
    "\n",
    "Adding positional encodings to the embeddings:\n",
    "\n",
    "```\n",
    "Token 1: [0.2, 0.1, 0.4, 0.7] + [0.1, 0.0, 0.1, 0.0] = [0.3, 0.1, 0.5, 0.7]\n",
    "Token 2: [0.9, 0.3, 0.2, 0.8] + [0.0, 0.1, 0.0, 0.1] = [0.9, 0.4, 0.2, 0.9]\n",
    "Token 3: [0.4, 0.5, 0.6, 0.1] + [0.1, 0.1, 0.1, 0.1] = [0.5, 0.6, 0.7, 0.2]\n",
    "```\n",
    "\n",
    "### Step 4: Self-Attention Mechanism\n",
    "Next, we compute the self-attention, $softmax(\\frac{QK^T}{\\sqrt{d_k}})V$. We need queries $Q$, keys $K$, and values $V$. Assume we use the same matrices for simplicity:\n",
    "\n",
    "```\n",
    "Query matrix $W_Q$, Key matrix $W_K$, Value matrix $W_V$:\n",
    "W_Q = W_K = W_V = Identity matrix (for simplicity)\n",
    "```\n",
    "\n",
    "Thus, our queries, keys, and values are the same as the positional encodings added embeddings:\n",
    "\n",
    "```\n",
    "Q = K = V = [[0.3, 0.1, 0.5, 0.7],\n",
    "             [0.9, 0.4, 0.2, 0.9],\n",
    "             [0.5, 0.6, 0.7, 0.2]]\n",
    "```\n",
    "\n",
    "We calculate the attention scores using the dot product of queries and keys, scaled by the square root of the embedding dimension (√4 = 2):\n",
    "\n",
    "```\n",
    "Attention scores = Q * K^T / √4\n",
    "                 = [[0.3, 0.1, 0.5, 0.7] dot [0.3, 0.1, 0.5, 0.7],\n",
    "                    [0.3, 0.1, 0.5, 0.7] dot [0.9, 0.4, 0.2, 0.9],\n",
    "                    [0.3, 0.1, 0.5, 0.7] dot [0.5, 0.6, 0.7, 0.2],\n",
    "                    ... (and so on for other tokens)] / 2\n",
    "\n",
    "Attention scores = [[1.26, 1.35, 0.82],\n",
    "                    [1.35, 1.82, 1.30],\n",
    "                    [0.82, 1.30, 0.94]] / 2\n",
    "\n",
    "Attention scores = [[0.63, 0.68, 0.41],\n",
    "                    [0.68, 0.91, 0.65],\n",
    "                    [0.41, 0.65, 0.47]]\n",
    "```\n",
    "\n",
    "Apply softmax to these scores to get the attention weights:\n",
    "\n",
    "```\n",
    "Attention weights = softmax(attention scores)\n",
    "                   ≈ [[0.350 , 0.368 , 0.281],\n",
    "                      [0.310 , 0.390 , 0.301],\n",
    "                      [0.300 , 0.382 , 0.318]]\n",
    "\n",
    "```\n",
    "\n",
    "We then compute the new representations for each token as a weighted sum of the values:\n",
    "\n",
    "```\n",
    "New representations = attention weights * V\n",
    "                    ≈  [0.350 , 0.368 , 0.281] dot [[0.3, 0.1, 0.5, 0.7],\n",
    "                       [0.310 , 0.390 , 0.301] dot [0.9, 0.4, 0.2, 0.9],\n",
    "                       [0.300 , 0.382 , 0.318] dot [0.5, 0.6, 0.7, 0.2]]\n",
    "\n",
    "New representations ≈ [[0.5767,0.3508,0.4453,0.6324],\n",
    "                       [0.5945,0.3676,0.4437,0.6282],\n",
    "                       [0.5928,0.3736,0.449,0.6174]]\n",
    "```\n",
    "Explain: When you compute the attention scores by taking the dot product of the queries and keys, and then applying a softmax, you get the attention weights.These attention weights indicate how much focus each token should give to every other token. By applying these weights to the values, the model constructs a new representation for each token that incorporates contextual information from other relevant tokens.\n",
    "\n",
    "### Step 5: Feed-Forward Network and Residual Connections\n",
    "Each of these new representations is then passed through a feed-forward neural network layer and added with the original representations (residual connections) for stabilization that means in case we encounter the weights near 0, we still make gradient to flow and the original information of tokens has been directly captured. Suppose for simplicity FFN=I.\n",
    "\n",
    "```\n",
    "Final representation for token 1:\n",
    "FFN([0.5767,0.3508,0.4453,0.6324]) + [0.3, 0.1, 0.5, 0.7]\n",
    "\n",
    "Final representation for token 2:\n",
    "FFN([0.5945,0.3676,0.4437,0.6282]) + [0.9, 0.4, 0.2, 0.9]\n",
    "\n",
    "Final representation for token 3:\n",
    "FFN([0.5928,0.3736,0.449,0.6174]) + [0.5, 0.6, 0.7, 0.2]\n",
    "\n",
    "=[[0.8767,0.4508,0.9453,1.3324],\n",
    "[1.4945,0.7676,0.6437,1.5282],\n",
    "[1.0928,0.9736,1.149,0.8174]]\n",
    "```\n",
    "Explain: \n",
    "1.Enhanced Expressiveness:\n",
    "By applying non-linear transformations, the FFN enhances the model's ability to learn complex patterns and dependencies within the data.\n",
    "\n",
    "2.Dimensionality Management:\n",
    "The two-step linear transformation allows the model to expand and contract the dimensionality of representations, providing flexibility in capturing various aspects of the input data.\n",
    "\n",
    "3.Efficiency:\n",
    "The FFN operates independently on each token, making it highly efficient and suitable for parallel computation on modern hardware.\n",
    "\n",
    "### Step 6: Pooling for Sentence Embedding\n",
    "Finally, we can aggregate these token embeddings into a single sentence embedding using mean pooling:\n",
    "\n",
    "```\n",
    "Sentence embedding = Mean([final_rep_1, final_rep_2, final_rep_3])\n",
    "                   ≈ Mean([[0.8767,0.4508,0.9453,1.3324],\n",
    "                       [1.4945,0.7676,0.6437,1.5282],\n",
    "                       [1.0928,0.9736,1.149,0.8174]])\n",
    "                   ≈ [1.1547, 0.7307, 0.9127, 1.2260]\n",
    "\n",
    "```\n",
    "Explain:\n",
    "1.Dimensionality Reduction: \n",
    "Mean pooling reduces the number of vectors by summarizing them into a single vector, making it easier to handle and process further.\n",
    "2.Global Context: \n",
    "It helps capture the global context of a sequence by averaging the representations of all tokens, thereby providing a holistic representation of the entire sequence.\n",
    "3.Stability: \n",
    "Averaging can reduce noise and variability in the representations, leading to more stable and generalized embeddings.\n",
    "\n",
    "### Summary\n",
    "The sentence \"I love NLP\" is transformed into the numerical vector [1.1547, 0.7307, 0.9127, 1.2260] through a series of steps involving tokenization, embedding, positional encoding, self-attention, feed-forward neural networks, and pooling. Each step incorporates both syntactic and semantic information, resulting in a comprehensive representation of the sentence in numerical form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1149b",
   "metadata": {},
   "source": [
    "## Remark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d6e40",
   "metadata": {},
   "source": [
    "### 1. Pre-trained are Ready to Use\n",
    "\n",
    "Pre-trained models are trained on large datasets and are ready to use for various natural language processing (NLP) tasks. These models have already learned rich representations of language, which can be directly applied to tasks or fine-tuned for specific applications with relatively small additional training data.\n",
    "\n",
    "### 2. General Framework for Training a Pre-trained Model\n",
    "\n",
    "To understand how these models are trained, let's break down the process into a simple framework. We'll use BERT (Bidirectional Encoder Representations from Transformers) as an example, but the general steps apply to other models like GPT, Word2Vec, and GloVe as well.\n",
    "\n",
    "#### Step 1: Data Collection\n",
    "\n",
    "1. **Large Corpus**: Gather a large and diverse corpus of text data. For BERT, this includes the entirety of English Wikipedia and the BookCorpus dataset.\n",
    "   \n",
    "#### Step 2: Preprocessing\n",
    "\n",
    "1. **Tokenization**: Convert text into tokens. BERT uses a WordPiece tokenizer, which breaks words into subwords and characters if necessary.\n",
    "2. **Formatting**: Format the text into sentences and pairs of sentences, as BERT also uses the next sentence prediction (NSP) task.\n",
    "\n",
    "#### Step 3: Model Architecture\n",
    "\n",
    "1. **Transformer Encoder**: Build the Transformer model architecture. BERT uses a multi-layer bidirectional Transformer encoder. Each layer has self-attention mechanisms and feed-forward neural networks.\n",
    "   \n",
    "#### Step 4: Training Objectives\n",
    "\n",
    "1. **Masked Language Model (MLM)**: Randomly mask some tokens in the input and train the model to predict these masked tokens. This teaches the model to understand context.\n",
    "   \n",
    "   \\[\n",
    "   \\mathcal{L}_{\\text{MLM}} = -\\sum_{i \\in \\text{masked positions}} \\log P_{\\text{model}}(w_i | \\text{context})\n",
    "   \\]\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**: Train the model to predict if one sentence follows another. This helps the model understand relationships between sentences.\n",
    "\n",
    "   \\[\n",
    "   \\mathcal{L}_{\\text{NSP}} = - (y \\log P_{\\text{model}}(y|A, B) + (1-y) \\log (1 - P_{\\text{model}}(y|A, B)))\n",
    "   \\]\n",
    "\n",
    "#### Step 5: Training Process\n",
    "\n",
    "1. **Initialization**: Initialize the model's weights, usually with some form of random initialization or pre-existing embeddings.\n",
    "2. **Forward Pass**: Pass the input tokens through the model to get predictions for masked tokens and next sentence relationships.\n",
    "3. **Compute Loss**: Calculate the loss using MLM and NSP objectives.\n",
    "4. **Backward Pass**: Use backpropagation to compute gradients of the loss with respect to the model parameters.\n",
    "5. **Optimization**: Update the model parameters using an optimization algorithm like Adam or AdamW.\n",
    "6. **Iteration**: Repeat the forward and backward passes for multiple epochs until the model converges.\n",
    "\n",
    "#### Step 6: Fine-tuning\n",
    "\n",
    "1. **Task-Specific Data**: Gather data for the specific task you want to fine-tune the model on (e.g., sentiment analysis, question answering).\n",
    "2. **Adaptation**: Train the pre-trained model further on this task-specific data, usually with a smaller learning rate and fewer epochs.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The framework for training a pre-trained model like BERT involves:\n",
    "1. Collecting a large and diverse text corpus.\n",
    "2. Preprocessing the text into tokens and formatted inputs.\n",
    "3. Defining the model architecture with Transformers.\n",
    "4. Setting up training objectives (MLM and NSP).\n",
    "5. Training the model through forward and backward passes with optimization.\n",
    "6. Fine-tuning the model on specific tasks.\n",
    "\n",
    "This process ensures that the model learns rich language representations that can be effectively used or adapted for a wide range of NLP tasks.\n",
    "\n",
    "### References\n",
    "1. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**: Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). [Paper](https://arxiv.org/abs/1810.04805)\n",
    "2. **Attention Is All You Need**: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). [Paper](https://arxiv.org/abs/1706.03762)\n",
    "3. **BERT Explained: State of the art language model for NLP**: Jay Alammar. [Blog Post](http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba432fa",
   "metadata": {},
   "source": [
    "# Task 1 (Homogeneous/Nonhomogeneous Method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c02011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from sentence-transformers) (1.10.0)\n",
      "Requirement already satisfied: tqdm in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: Pillow in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from sentence-transformers) (1.2.1)\n",
      "Collecting transformers<5.0.0,>=4.34.0\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\n",
      "Collecting huggingface-hub>=0.15.1\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (22.0)\n",
      "Requirement already satisfied: requests in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2022.7.9)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-macosx_10_12_x86_64.whl (415 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.8/415.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-macosx_10_12_x86_64.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jiahuasong/anaconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.11.0\n",
      "    Uninstalling fsspec-2022.11.0:\n",
      "      Successfully uninstalled fsspec-2022.11.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.10.1\n",
      "    Uninstalling huggingface-hub-0.10.1:\n",
      "      Successfully uninstalled huggingface-hub-0.10.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.11.4\n",
      "    Uninstalling tokenizers-0.11.4:\n",
      "      Successfully uninstalled tokenizers-0.11.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.24.0\n",
      "    Uninstalling transformers-4.24.0:\n",
      "      Successfully uninstalled transformers-4.24.0\n",
      "Successfully installed fsspec-2024.6.1 huggingface-hub-0.23.5 safetensors-0.4.3 sentence-transformers-3.0.1 tokenizers-0.19.1 transformers-4.42.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f171ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.17897401e-02 -2.12560762e-02 -3.86042371e-02  1.83207504e-02\n",
      " -9.86111686e-02 -3.97454463e-02  5.58330826e-02  1.24115478e-02\n",
      "  8.01739544e-02  8.03998634e-02  5.67718083e-03 -2.06915289e-02\n",
      " -2.66179685e-02  8.56194645e-02  2.99868807e-02 -1.40585182e-02\n",
      " -2.19038092e-02  1.90649796e-02 -6.31705374e-02 -7.73281083e-02\n",
      " -1.32520303e-01  6.43389719e-03  1.84917375e-02 -1.28093141e-03\n",
      "  3.47857624e-02  5.92514165e-02 -1.63243469e-02 -2.99711954e-02\n",
      "  8.06165580e-03 -5.63003682e-02 -1.13594100e-01  7.57678673e-02\n",
      "  3.39605622e-02 -4.19125371e-02 -6.94662556e-02 -2.22117957e-02\n",
      "  5.36275543e-02  2.95904931e-03  2.93243397e-02  6.34898022e-02\n",
      " -2.83782165e-02 -2.38896236e-02  1.33931577e-01  8.16639513e-03\n",
      " -3.41529995e-02 -3.80521826e-02 -9.32704378e-03 -1.71557330e-02\n",
      "  7.44805709e-02  5.93681633e-02  4.07176204e-02  4.63434681e-02\n",
      " -1.48743019e-01  6.47118874e-03  5.75322509e-02  3.50241289e-02\n",
      " -1.83352698e-02  4.36619520e-02  2.92016305e-02 -7.08216205e-02\n",
      " -2.81906500e-02  2.93917246e-02  5.00018708e-02 -2.44709793e-02\n",
      "  6.61187097e-02 -3.04790698e-02 -2.90218126e-02  2.61169355e-02\n",
      " -5.47342077e-02  6.42752051e-02  1.99495703e-02  9.16129798e-02\n",
      "  4.54432555e-02  6.25592172e-02  1.32404089e-01  1.42811183e-02\n",
      "  9.80032724e-04  3.21648233e-02  1.45781096e-02  3.76791619e-02\n",
      "  2.60392986e-02 -8.96034911e-02 -5.25753945e-02 -2.55806930e-03\n",
      "  2.68042479e-02 -1.04150608e-01  1.91974603e-02  4.65624630e-02\n",
      "  6.50878670e-03 -8.31582546e-02  6.36944771e-02 -4.49973792e-02\n",
      " -4.64135297e-02  4.63219061e-02 -2.35244054e-02  6.44053593e-02\n",
      "  2.11804975e-02  9.21118259e-03 -3.28140184e-02  1.20974563e-01\n",
      "  4.21137102e-02  1.11263404e-02 -3.86530347e-02  1.01050541e-01\n",
      " -9.11337230e-03 -8.55449704e-04  2.27290615e-02  1.02317920e-02\n",
      "  5.09198643e-02 -5.20198978e-02 -2.87575624e-03 -1.85678992e-02\n",
      "  1.43212844e-02  9.38936602e-03  3.60680707e-02  3.74903120e-02\n",
      "  9.96116251e-02  2.88885254e-02  9.64941457e-02  1.89604179e-03\n",
      "  9.77993477e-03  7.53958598e-02  7.21563911e-03 -2.37502530e-02\n",
      " -1.65045410e-02  4.27236641e-03  1.91742682e-03 -5.49381464e-33\n",
      " -1.12202697e-01 -3.80303711e-02  1.20518813e-02  2.80205570e-02\n",
      " -8.32711440e-03  2.58939862e-02  1.24278683e-02  3.02707516e-02\n",
      "  4.07030173e-02  6.98314933e-03 -2.33945344e-02  9.64143127e-02\n",
      " -2.11451408e-02  7.61414319e-02  1.01948172e-01 -4.09254320e-02\n",
      "  3.14697251e-02  3.76966670e-02 -8.68375972e-02 -1.10056316e-02\n",
      " -2.97193620e-02 -1.77695304e-02  2.19647377e-03  3.91339809e-02\n",
      "  2.31764894e-02  1.19635537e-02  4.75847609e-02 -4.80469465e-02\n",
      "  4.39927615e-02  1.72655378e-02 -1.17167588e-02  6.71748221e-02\n",
      " -1.03933781e-01 -2.85756756e-02  6.76743779e-03 -9.09161419e-02\n",
      "  6.94907596e-03 -2.60190805e-03  4.14405204e-02  7.57972302e-04\n",
      "  6.60492480e-03 -1.26109524e-02  1.30580449e-02 -1.10064885e-02\n",
      "  4.85587642e-02  7.56536126e-02  6.05609939e-02  3.85250598e-02\n",
      "  4.02759761e-02 -4.53075320e-02 -8.03026557e-02 -7.94020891e-02\n",
      "  1.25290519e-02 -8.69255792e-03  3.96796390e-02 -1.53024485e-02\n",
      "  1.83584094e-02  1.72436628e-02 -3.57523039e-02  1.65622600e-03\n",
      " -4.50983196e-02  1.00199752e-01  6.71089888e-02 -7.92267993e-02\n",
      " -1.22834504e-01  6.62321523e-02 -9.05314907e-02 -9.52749106e-04\n",
      "  5.22473939e-02  1.14061851e-02 -7.94967338e-02  3.50883193e-02\n",
      " -4.08273414e-02  1.86273381e-02 -5.69315329e-02 -4.31919508e-02\n",
      "  8.46147016e-02 -1.57458242e-02 -3.13817933e-02 -2.06638537e-02\n",
      "  5.55991270e-02 -6.63689747e-02  6.25620633e-02 -9.65766460e-02\n",
      "  4.44136076e-02  1.79607961e-02 -1.72924809e-03 -1.11833632e-01\n",
      "  1.03493156e-02 -7.80947953e-02 -2.51854192e-02 -5.49653731e-02\n",
      "  2.29343995e-02  2.91736443e-02 -2.87146177e-02  3.70782657e-33\n",
      " -1.24706462e-01 -1.90464929e-02 -9.00149718e-02  5.98982535e-02\n",
      " -2.47080699e-02 -2.71320362e-02 -2.69002914e-02 -1.92736927e-02\n",
      "  6.97145686e-02  3.19511071e-02  6.72394335e-02 -2.12483350e-02\n",
      " -4.91482615e-02  5.35840131e-02 -4.12185043e-02 -2.67014224e-02\n",
      " -4.46322709e-02 -3.94615754e-02 -8.28199983e-02  3.09593719e-03\n",
      " -4.49567102e-02  5.76050729e-02 -4.13164496e-02 -1.25003131e-02\n",
      "  2.55319066e-02 -3.67416348e-03 -3.85073461e-02 -4.15338352e-02\n",
      " -3.95106599e-02  7.13312179e-02 -1.82841234e-02 -9.74874273e-02\n",
      "  2.82843132e-02 -9.47977379e-02 -7.90095478e-02 -4.45673652e-02\n",
      "  2.04564235e-03 -1.26189888e-02 -5.43528460e-02 -2.40246207e-02\n",
      "  1.45732490e-02 -7.20134526e-02  1.06940992e-01  3.93362716e-02\n",
      "  1.03722821e-04  3.76778953e-02  9.60928500e-02  3.19215059e-02\n",
      "  4.09034006e-02  5.11476956e-02 -1.05127450e-02  1.44449798e-02\n",
      " -2.71568727e-03 -4.49491404e-02  3.40677574e-02 -7.22751766e-03\n",
      " -2.31482908e-02  1.04712844e-02 -6.21713251e-02 -5.65861305e-03\n",
      " -3.86684649e-02  8.12130123e-02 -5.31659694e-03  9.57864374e-02\n",
      " -3.12147886e-02 -4.82473373e-02 -4.37146947e-02 -5.10793254e-02\n",
      " -9.89230648e-02 -3.53976563e-02 -1.83262900e-02  1.75866578e-02\n",
      " -3.81999947e-02  4.27537933e-02 -7.80541003e-02 -1.40763391e-02\n",
      " -9.68215172e-04  1.07534744e-01  3.38433916e-03 -6.38553966e-03\n",
      "  2.48866584e-02 -4.86232154e-02 -5.88820949e-02  2.67235395e-02\n",
      " -7.26280138e-02  2.27553491e-02  4.71387170e-02  1.27438661e-02\n",
      "  5.99823520e-03 -5.78615852e-02  7.30928546e-03  5.19152693e-02\n",
      " -2.93636434e-02 -4.97340448e-02  3.78569923e-02 -1.33396201e-08\n",
      "  8.83895031e-04  5.34891710e-03  5.88888116e-02 -8.70249942e-02\n",
      "  1.00157214e-02  6.27688318e-02 -5.99768609e-02  1.12806736e-02\n",
      " -5.87035567e-02 -2.43044067e-02  1.00432619e-01  5.81749678e-02\n",
      " -7.41021484e-02  2.19211057e-02  4.08709273e-02 -3.69970761e-02\n",
      "  1.15944438e-01 -5.13981543e-02 -8.40902887e-03  5.88258263e-03\n",
      " -8.87246244e-03  3.61003652e-02  1.53581295e-02  3.48936245e-02\n",
      " -5.84534146e-02  4.09821570e-02  8.13968554e-02  4.59832363e-02\n",
      "  1.82976518e-02  3.67001258e-02 -1.35655170e-02  1.32697886e-02\n",
      " -4.25202698e-02  1.91185549e-02 -4.91904281e-02  3.58221424e-03\n",
      "  3.72391716e-02 -3.89318503e-02  3.35726589e-02 -7.04787597e-02\n",
      " -3.34051922e-02 -9.40916035e-03  2.54208967e-02 -5.68472855e-02\n",
      "  5.88119356e-03  1.16646020e-02  7.57930949e-02 -5.75176366e-02\n",
      " -1.33529957e-02  4.34547290e-02  5.24305664e-02  2.61524301e-02\n",
      " -6.02909364e-03 -1.64777134e-02  9.64913815e-02 -6.52663484e-02\n",
      " -6.10932782e-02  6.12156559e-03 -7.01806918e-02  4.90521491e-02\n",
      "  1.47673786e-01  3.99467386e-02  3.77936624e-02 -1.09718293e-02]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define your sentence\n",
    "sentence = \"I like math\"\n",
    "\n",
    "# Transform the sentence into a numerical vector\n",
    "sentence_embedding = model.encode(sentence)\n",
    "\n",
    "# Print the resulting vector\n",
    "print(sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cab753c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4510c17f",
   "metadata": {},
   "source": [
    "# Homogeneous & Nonhomogeneous Experiment Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b38b9",
   "metadata": {},
   "source": [
    "Using Singular Value Decomposition (SVD) on the embedding matrix of sentences can be a viable method to analyze and compare the similarity of texts. SVD is a mathematical technique that decomposes a matrix into three components: U, Σ (Sigma), and V^T. The singular values (diagonal elements of Σ) capture essential features of the data, and their properties can indeed be used to compare similarities between texts.\n",
    "\n",
    "Here's how you can apply SVD to sentence embeddings and compare their similarities:\n",
    "\n",
    "### Steps to Use SVD for Comparing Sentence Similarity\n",
    "\n",
    "1. **Generate Embedding Matrix**:\n",
    "   Generate the embedding matrix for each sentence using a pre-trained model like `sentence-transformers`. Each row of the matrix corresponds to the embedding of a token or the entire sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bf54a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define sentences\n",
    "sentences = [\"Sentence one.\", \"Sentence two.\", \"Another sentence.\"]\n",
    "\n",
    "# Encode sentences into embeddings\n",
    "embeddings = model.encode(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5c68e",
   "metadata": {},
   "source": [
    "2. **Apply SVD**:\n",
    "   Apply SVD to the embedding matrix to decompose it into U, Σ, and V^T.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73863e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Perform SVD\n",
    "U, Sigma, Vt = svd(embeddings, full_matrices=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395c8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6f98dfc",
   "metadata": {},
   "source": [
    "3. **Compare Singular Values**:\n",
    "   Compare the singular values (Σ) of the sentences. Singular values contain characteristic information about the sentences, and comparing them can help in determining the similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78a08e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between sentences: 1.1680207\n"
     ]
    }
   ],
   "source": [
    "def compare_singular_values(Sigma1, Sigma2):\n",
    "    # A simple distance metric (e.g., Euclidean distance) between singular values\n",
    "    distance = np.linalg.norm(Sigma1 - Sigma2)\n",
    "    return distance\n",
    "\n",
    "# Example: Compare singular values of first two sentences\n",
    "distance = compare_singular_values(Sigma[0], Sigma[1])\n",
    "print(\"Distance between sentences:\", distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb3caa",
   "metadata": {},
   "source": [
    "### Why SVD Can Be Useful\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   SVD reduces the dimensions of the embedding matrix while preserving important information, making it easier to analyze and compare.\n",
    "\n",
    "2. **Capturing Variance**:\n",
    "   The singular values capture the variance in the data. Similar texts are likely to have similar variance distributions, reflected in their singular values.\n",
    "\n",
    "3. **Noise Reduction**:\n",
    "   SVD helps in filtering out noise and retaining significant features, improving the quality of similarity comparison.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- **Appropriate Metric**: Choose an appropriate distance metric (e.g., Euclidean, cosine) to compare the singular values effectively.\n",
    "- **Normalization**: Ensure embeddings are normalized before applying SVD to improve the reliability of the comparison.\n",
    "- **Contextual Information**: While SVD captures significant features, consider using additional contextual information for a more comprehensive similarity analysis.\n",
    "\n",
    "### Example Code\n",
    "\n",
    "Here's a complete example using the `sentence-transformers` library and SVD:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "236b71a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between first and second sentences: 0.5522927\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define sentences\n",
    "sentences = [\"I love natural language processing.\", \n",
    "             \"Natural language processing is fascinating.\", \n",
    "             \"Machine learning is a subset of AI.\"]\n",
    "\n",
    "# Encode sentences into embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Perform SVD on the embedding matrix\n",
    "U, Sigma, Vt = svd(embeddings, full_matrices=False)\n",
    "\n",
    "# Function to compare singular values\n",
    "def compare_singular_values(Sigma1, Sigma2):\n",
    "    return np.linalg.norm(Sigma1 - Sigma2)\n",
    "\n",
    "# Compare singular values of the first two sentences\n",
    "distance = compare_singular_values(Sigma[0], Sigma[1])\n",
    "print(\"Distance between first and second sentences:\", distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "875d857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between first and second sentences: 1.0492992\n"
     ]
    }
   ],
   "source": [
    "distance = compare_singular_values(Sigma[0], Sigma[2])\n",
    "print(\"Distance between first and third sentences:\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "789435d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between first and second sentences: 0.49700657\n"
     ]
    }
   ],
   "source": [
    "distance = compare_singular_values(Sigma[1], Sigma[2])\n",
    "print(\"Distance between second and third sentences:\", distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f219402",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Using SVD in this manner provides a mathematical approach to analyzing and comparing sentence similarities, leveraging the characteristic information captured in the singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f2438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40b096cf",
   "metadata": {},
   "source": [
    "# Task2: Influence Method building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c0bc8",
   "metadata": {},
   "source": [
    "## 1.Building a Text Tranformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf768ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between the first and third sentences: 0.8412186\n",
      "Shape of the embeddings tensor: (10, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define a paragraph\n",
    "paragraph = \"\"\"I love Columbia. \n",
    "I love humans. \n",
    "I hate machines.\n",
    "The weather is great today. \n",
    "AI technology is evolving rapidly. \n",
    "Hello World.  \n",
    "Hello World.  Hello World. I eat something. I learn the PDE with great interests.  \"\"\"\n",
    "\n",
    "# Split the paragraph into sentences\n",
    "sentences = paragraph.split('. ')\n",
    "sentences = [s.strip() for s in sentences if s]\n",
    "\n",
    "# Encode sentences into embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Create a tensor (3D array) from embeddings\n",
    "embeddings_tensor = np.array(embeddings)\n",
    "\n",
    "# Perform SVD on the embeddings tensor (treating it as a matrix)\n",
    "U, Sigma, Vt = svd(embeddings_tensor, full_matrices=False)\n",
    "\n",
    "# Function to compare singular values\n",
    "def compare_singular_values(Sigma1, Sigma2):\n",
    "    return np.linalg.norm(Sigma1 - Sigma2)\n",
    "\n",
    "# Compare singular values of the first and third sentences\n",
    "distance = compare_singular_values(Sigma[0], Sigma[2])\n",
    "print(\"Distance between the first and third sentences:\", distance)\n",
    "\n",
    "# Display the embeddings tensor shape\n",
    "print(\"Shape of the embeddings tensor:\", embeddings_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1ff06b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0523927 , -0.02771043,  0.07801216, ..., -0.0483209 ,\n",
       "        -0.00326195, -0.02432846],\n",
       "       [-0.03149126,  0.04724425,  0.10051548, ...,  0.04358653,\n",
       "        -0.004703  , -0.06739449],\n",
       "       [-0.02798782,  0.08863726,  0.14882033, ...,  0.03650202,\n",
       "        -0.08428912,  0.03299301],\n",
       "       ...,\n",
       "       [-0.01602481, -0.03562249, -0.00661822, ...,  0.03746296,\n",
       "         0.00435875, -0.10044489],\n",
       "       [ 0.0210747 , -0.03587139,  0.07484306, ...,  0.04798881,\n",
       "        -0.04538018,  0.01351546],\n",
       "       [-0.11883838,  0.04829872, -0.00254814, ...,  0.1264095 ,\n",
       "         0.04654904, -0.01571722]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5ff4dbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.8861103e+00, 1.2327874e+00, 1.0448917e+00, 9.7111142e-01,\n",
       "       9.0963864e-01, 8.5888487e-01, 8.3260292e-01, 7.9347944e-01,\n",
       "       1.6720615e-07, 1.6957998e-08], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3279dc",
   "metadata": {},
   "source": [
    "## 2.Building a Sentences Input Function of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8cb5f2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values at t=0: [1.0000001]\n",
      "Singular values at t=1: [1.1186045 0.8652884]\n",
      "Singular values at t=2: [1.1990715  0.94818205 0.8143583 ]\n",
      "Singular values at t=3: [1.2409093  0.97366    0.92579126 0.8093466 ]\n",
      "Singular values at t=4: [1.284712   1.062413   0.9412122  0.82503957 0.80884117]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def update_singular_values(initial_text, new_sentences):\n",
    "    # Initialize the list of singular values\n",
    "    singular_values_over_time = []\n",
    "\n",
    "    # Start with the initial text\n",
    "    current_text = initial_text\n",
    "    sentences = [current_text]\n",
    "    \n",
    "    # Encode the initial text\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Perform SVD on the initial embedding\n",
    "    U, Sigma, Vt = svd(embeddings, full_matrices=False)\n",
    "    \n",
    "    # Store the initial singular values\n",
    "    singular_values_over_time.append(Sigma)\n",
    "\n",
    "    # Iterate over the new sentences and update the text and singular values\n",
    "    for sentence in new_sentences:\n",
    "        # Add the new sentence to the current text\n",
    "        current_text += ' ' + sentence\n",
    "        sentences = current_text.split('. ')\n",
    "        sentences = [s.strip() for s in sentences if s]\n",
    "\n",
    "        # Encode the updated text\n",
    "        embeddings = model.encode(sentences)\n",
    "\n",
    "        # Perform SVD on the updated embedding matrix\n",
    "        U, Sigma, Vt = svd(embeddings, full_matrices=False)\n",
    "\n",
    "        # Store the new singular values\n",
    "        singular_values_over_time.append(Sigma)\n",
    "\n",
    "    return singular_values_over_time\n",
    "\n",
    "# Example usage\n",
    "initial_text = \"I love Columbia.\"\n",
    "new_sentences = [\"I love humans.\", \"I hate machines.\", \"The weather is great today.\", \"AI technology is evolving rapidly.\"]\n",
    "\n",
    "singular_values = update_singular_values(initial_text, new_sentences)\n",
    "\n",
    "# Print the singular values over time\n",
    "for t, sv in enumerate(singular_values):\n",
    "    print(f\"Singular values at t={t}: {sv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe56b6f",
   "metadata": {},
   "source": [
    "## 3.Building a Texts Input Function of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "31649d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values for sentence 0: [1.4157951, 1.5523776, 1.6705251, 1.7835956, 1.9310893]\n",
      "Singular values for sentence 1: [0.97942734, 1.0803684, 1.2531228, 1.2610172, 1.4467579]\n",
      "Singular values for sentence 2: [0.78083444, 1.0289794, 1.0514475, 1.2290188, 1.2356913]\n",
      "Singular values for sentence 3: [0.65310305, 0.84991807, 0.9844411, 1.0432652, 1.0727606]\n",
      "Singular values for sentence 4: [0.78879017, 0.8868744, 0.98082227, 1.0369047]\n",
      "Singular values for sentence 5: [0.7797037, 0.83658004, 0.89690244, 1.0022436]\n",
      "Singular values for sentence 6: [0.64159334, 0.8107564, 0.8373598, 0.8608565]\n",
      "Singular values for sentence 7: [0.75743496, 0.813528, 0.82794476]\n",
      "Singular values for sentence 8: [0.69202507, 0.7889265, 0.81092745]\n",
      "Singular values for sentence 9: [0.6066402, 0.7608457, 0.7987572]\n",
      "Singular values for sentence 10: [0.7041629, 0.77759594]\n",
      "Singular values for sentence 11: [0.66815263, 0.7229057]\n",
      "Singular values for sentence 12: [0.597262, 0.6796976]\n",
      "Singular values for sentence 13: [0.64956]\n",
      "Singular values for sentence 14: [0.61950284]\n",
      "Singular values for sentence 15: [0.5506657]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def update_singular_values(initial_paragraph, new_paragraphs):\n",
    "    # Initialize the list of singular values for each sentence\n",
    "    singular_values_over_time = []\n",
    "\n",
    "    # Start with the initial paragraph\n",
    "    current_text = initial_paragraph\n",
    "    sentences = current_text.split('. ')\n",
    "    sentences = [s.strip() for s in sentences if s]\n",
    "    \n",
    "    # Encode the initial sentences\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Perform SVD on the initial embedding\n",
    "    U, Sigma, Vt = svd(embeddings, full_matrices=False)\n",
    "    \n",
    "    # Store the initial singular values for each sentence\n",
    "    for sigma in Sigma:\n",
    "        singular_values_over_time.append([sigma])\n",
    "\n",
    "    # Iterate over the new paragraphs and update the text and singular values\n",
    "    for paragraph in new_paragraphs:\n",
    "        # Add the new paragraph to the current text\n",
    "        current_text += ' ' + paragraph\n",
    "        sentences = current_text.split('. ')\n",
    "        sentences = [s.strip() for s in sentences if s]\n",
    "\n",
    "        # Encode the updated sentences\n",
    "        embeddings = model.encode(sentences)\n",
    "\n",
    "        # Perform SVD on the updated embedding matrix\n",
    "        U, Sigma, Vt = svd(embeddings, full_matrices=False)\n",
    "\n",
    "        # Update the singular values for each sentence\n",
    "        for i, sigma in enumerate(Sigma):\n",
    "            if i < len(singular_values_over_time):\n",
    "                singular_values_over_time[i].append(sigma)\n",
    "            else:\n",
    "                singular_values_over_time.append([sigma])\n",
    "\n",
    "    return singular_values_over_time\n",
    "\n",
    "# Example usage\n",
    "initial_paragraph = \"I love Columbia. The campus is beautiful. The faculty is outstanding. Students are very friendly.\"\n",
    "new_paragraphs = [\n",
    "    \"I love humans. People are kind and compassionate. Society is complex.\",\n",
    "    \"I hate machines. Technology can be overwhelming. AI can be intrusive.\",\n",
    "    \"The weather is great today. Sunshine is perfect. The temperature is just right.\",\n",
    "    \"AI technology is evolving rapidly. Innovations are happening every day. It's changing our world.\"\n",
    "]\n",
    "\n",
    "singular_values = update_singular_values(initial_paragraph, new_paragraphs)\n",
    "\n",
    "# Print the singular values over time\n",
    "for t, sv in enumerate(singular_values):\n",
    "    print(f\"Singular values for sentence {t}: {sv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec9215",
   "metadata": {},
   "source": [
    "#### test for above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cf010b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4157951 , 0.97942734, 0.78083444, 0.65310305], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"I love Columbia. The campus is beautiful. The faculty is outstanding. Students are very friendly.\"\n",
    "k=s.split('. ')\n",
    "e = model.encode(k)\n",
    "Sigma = svd(e, full_matrices=False)\n",
    "Sigma[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc203212",
   "metadata": {},
   "source": [
    "## 4.Reaction Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95416ec",
   "metadata": {},
   "source": [
    "This is one of the innovative parts of our research. It aims to capture the reaction based on some indicators. For example, in stock market, we have text for public to influence the stock price. We make everything else controlled so that the only thing of interets is that how the text influence the stock price. Now, the stock price is the indicator I mention, and it goes together with the corresponding text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82abe5",
   "metadata": {},
   "source": [
    "# [UNFINISHED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5795f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67fe688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749496b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4efcef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4d975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95916b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232a0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed7d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe5f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c4d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c590bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b6d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0485f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc8e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1831e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0b378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1c62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
